"""
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚Ä¢ —Å–æ–±–∏—Ä–∞–µ—Ç input:
    topics ‚Üí //‚Ä¶/topic_analysis
    posts  ‚Üí //‚Ä¶/post_summaries
‚Ä¢ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç prompt (—Å–º. –¢–ó) ‚Üí –≤—ã–∑—ã–≤–∞–µ—Ç LLM
‚Ä¢ –ø–∏—à–µ—Ç TOP-5 –≤ //‚Ä¶/daily_digest  (upsert)

DEPENDENCIES
  tg_etl.query_yql, tg_etl.upsert_df_to_yt
  eliza_client.eliza_chat
"""

from __future__ import annotations
import json, os, re, datetime as dt
from typing import Any, Dict, List

import pandas as pd
from dateutil import tz

from . import tg_etl
from . import eliza_client


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ YT —Ç–∞–±–ª–∏—Ü—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ROOT           = os.getenv("YT_ROOT", "//tmp/ia-nartov/hackathon")
TBL_TOPICS     = f"{ROOT}/topic_analysis"
TBL_POSTS      = f"{ROOT}/post_summaries"
TBL_CHATS      = f"{ROOT}/tg_chats"
TBL_OUT        = f"{ROOT}/daily_digest"

SCHEMA_OUT = [
    {"name": "digest_id",   "type": "string", "sort_order": "ascending"},
    {"name": "channel_id",  "type": "int64"},
    {"name": "date",        "type": "string"},
    {"name": "digest_text", "type": "string"}      # ‚Üê –≥–æ—Ç–æ–≤—ã–π –ø–æ—Å—Ç
]

MSK = tz.gettz("Europe/Moscow")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PROMPT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PROMPT_TMPL = """
–¢—ã ‚Äî –≥–ª–∞–≤–Ω—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞ –∫–∞–Ω–∞–ª–∞.
–ù–∞ –≤—Ö–æ–¥ —Ç—ã –ø–æ–ª—É—á–∞–µ—à—å –ø–µ—Ä–µ—á–µ–Ω—å —Ç–µ–º-–æ–±—Å—É–∂–¥–µ–Ω–∏–π –∏–∑ —á–∞—Ç–∞.
–¢–µ–±–µ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç —Å —Ä–∞–∑–±–∏–≤–∫–æ–π –Ω–∞ –æ–±—Å—É–∂–¥–µ–Ω–∏—è –∏ –∫–æ–º–º–∏—Ç—ã.

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üõà –í–•–û–î

–î–∞—Ç–∞: {date}
–ö–∞–Ω–∞–ª: {channel_name} ‚Äî {channel_description}

–¢–µ–º—ã –∑–∞ –¥–µ–Ω—å:
{input_payload}

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üìã –ó–ê–î–ê–ß–ê

1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ —Ç–µ–º—ã –∏ —Ä–∞–∑–¥–µ–ª–∏ –∏—Ö –Ω–∞ –¥–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:
   - **–û–ë–°–£–ñ–î–ï–ù–ò–Ø**: —Ç–µ–º—ã, –≥–¥–µ –ª—é–¥–∏ –æ–±—Å—É–∂–¥–∞–ª–∏ –≤–æ–ø—Ä–æ—Å—ã, –ø—Ä–æ–±–ª–µ–º—ã, –∏–¥–µ–∏
   - **–ö–û–ú–ú–ò–¢–´**: —Ç–µ–º—ã, –≥–¥–µ –∫—Ç–æ-—Ç–æ –æ–±–µ—â–∞–ª —á—Ç–æ-—Ç–æ —Å–¥–µ–ª–∞—Ç—å –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É –≤—Ä–µ–º–µ–Ω–∏

2. –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–π bullet-—Å–ø–∏—Å–æ–∫ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—É–Ω–∫—Ç–æ–≤
3. –ò—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ–ª–µ–π: status, conclusions, resume

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üì§ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ô –í–´–•–û–î

{{
  "discussions": [
    "‚Ä¢ –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è 1",
    "‚Ä¢ –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è 2"
  ],
  "commitments": [
    "‚Ä¢ –ö—Ç–æ –æ–±–µ—â–∞–ª —á—Ç–æ —Å–¥–µ–ª–∞—Ç—å –∫–æ–≥–¥–∞",
    "‚Ä¢ –î—Ä—É–≥–æ–π –∫–æ–º–º–∏—Ç —Å –¥–µ–¥–ª–∞–π–Ω–æ–º"
  ]
}}

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚ö†Ô∏è –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø
‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Ö–æ–¥–∞
‚Ä¢ –ö–∞–∂–¥—ã–π bullet –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –∏ –∫—Ä–∞—Ç–∫–∏–º
‚Ä¢ –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–º–º–∏—Ç–æ–≤ –∏–ª–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–π, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤
‚Ä¢ –í—ã—Ö–æ–¥ ‚Äî –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º JSON-—Ñ–æ—Ä–º–∞—Ç–µ.
"""


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _get_channel(channel_id: int) -> pd.Series:
    return tg_etl.query_yql(
    f"""SELECT chat, description
    FROM hahn.`{TBL_CHATS}`
    WHERE chat_id = {channel_id}
    LIMIT 1;"""
    ).iloc[0]

def _load_topics(channel_id: int, date: dt.date) -> list[dict]:
    """–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç [{status, conclusions, resume}, ‚Ä¶]"""
    df = tg_etl.query_yql(
        f"""
        SELECT status, conclusions, resume
        FROM hahn.`{TBL_TOPICS}`
        WHERE channel_id = {channel_id}
          AND date = "{date}"
        ORDER BY topic_id;
        """
    )
    return df.to_dict("records") if not df.empty else []

# def _load_posts(channel_id: int, date: dt.date) -> list[dict]:
#     df = query_yql(
#     f"""
#     SELECT time, short_summary, detailed_summary
#     FROM hahn.`{TBL_POSTS}`
#     WHERE channel_id = {channel_id}
#     AND toDate(post_ts) = Date("{date}")
#     ORDER BY post_ts;
#     """
#     )
#     if df.empty:
#         return []
        
#     df["detailed_summary"] = df["detailed_summary"].apply(json.loads)
#     return df.to_dict("records")

def _prompt(date: dt.date,
            channel: pd.Series,
            topics: list[dict]) -> List[Dict[str, str]]:
    
    payload = json.dumps(topics, ensure_ascii=False, indent=2)
    
    txt = PROMPT_TMPL.format(
        date=date,
        channel_name=channel["chat"],
        channel_description=channel["description"],
        input_payload=payload,
    )
    
    return [{"role": "user", "content": txt}]

def _parse_answer(raw: str) -> Dict[str, Any]:
    """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç."""
    try:
        data = json.loads(raw)
    except json.JSONDecodeError:
        cleaned = re.sub(r"```json|```", "", raw, flags=re.I).strip()
        data = json.loads(cleaned)

    return data

def _format_digest_text(digest_data: Dict[str, Any]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∞–π–¥–∂–µ—Å—Ç –≤ –∫—Ä–∞—Å–∏–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ YT."""
    lines = []
    
    if digest_data.get("discussions"):
        lines.append("üó£Ô∏è –û–ë–°–£–ñ–î–ï–ù–ò–Ø:")
        for item in digest_data["discussions"]:
            lines.append(f"  {item}")
        lines.append("")
    
    if digest_data.get("commitments"):
        lines.append("üìã –ö–û–ú–ú–ò–¢–´:")
        for item in digest_data["commitments"]:
            lines.append(f"  {item}")
    
    return "\n".join(lines)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main entry ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def run_daily_digester(
    date: dt.date,
    channel_id: int,
    *,
    model: str = "yandex",
    verify: bool | str = True,
    ) -> None:
    """–°–æ–±–∏—Ä–∞–µ—Ç –¥–Ω–µ–≤–Ω–æ–π –¥–∞–π–¥–∂–µ—Å—Ç –∏ –∫–ª–∞–¥—ë—Ç –≤ daily_digest."""
    channel = _get_channel(channel_id)
    topics  = _load_topics(channel_id, date)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if not topics:
        print("‚è≠  –ù–µ—á–µ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∏—Ç—å: –Ω–µ—Ç topics")
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
    meaningful_topics = [t for t in topics if t.get('resume') and len(str(t.get('resume', '')).strip()) > 10]
    if not meaningful_topics:
        print("‚è≠  –ù–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–µ–º –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–∞; –ø—Ä–æ–ø—É—Å–∫")
        return

    prompt = _prompt(date, channel, meaningful_topics)
    rsp = eliza_client.eliza_chat(prompt, model=model, verify=verify)
    raw_json = rsp["response"]["Responses"][0]["Response"]
    
    digest_data = _parse_answer(raw_json)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç
    if not digest_data or (not digest_data.get("discussions") and not digest_data.get("commitments")):
        print("‚è≠  –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –¥–∞–π–¥–∂–µ—Å—Ç; –ø—Ä–æ–ø—É—Å–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ YT")
        return
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ YT
    digest_text = _format_digest_text(digest_data)
    
    # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ —Ç–∞–±–ª–∏—á–∫–µ)
    print(f"\n{'='*60}")
    print(f"üìä –ï–ñ–ï–î–ù–ï–í–ù–´–ô –î–ê–ô–î–ñ–ï–°–¢ {date}")
    print(f"üì¢ –ö–∞–Ω–∞–ª: {channel['chat']}")
    print(f"{'='*60}")
    print(f"\n{digest_text}")
    print(f"\n{'='*60}")
    
    row = {
        "digest_id": f"{channel_id}_{date}",
        "channel_id": channel_id,
        "date": str(date),
        "digest_text": digest_text
    }
    tg_etl.upsert_df_to_yt(pd.DataFrame([row]), TBL_OUT, SCHEMA_OUT)
    print(f"‚úÖ daily digest ({date}) upsert ‚Üí {TBL_OUT}")